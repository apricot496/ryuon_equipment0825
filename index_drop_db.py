from __future__ import annotations

import sqlite3
from pathlib import Path
import pandas as pd

# ==== è¨­å®šï¼ˆãƒ‘ã‚¹äº‹æ•…é˜²æ­¢ï¼šã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜å ´æ‰€ã® equipment.db ã‚’å‚ç…§ï¼‰ ====
BASE_DIR = Path(__file__).resolve().parent
DB_PATH = BASE_DIR / "equipment.db"
LOG_PATH = BASE_DIR / "load_log.csv"


def table_exists(conn: sqlite3.Connection, name: str) -> bool:
    row = conn.execute(
        "SELECT 1 FROM sqlite_master WHERE type='table' AND name=? LIMIT 1",
        (name,),
    ).fetchone()
    return row is not None


def count_rows(conn: sqlite3.Connection, table_name: str) -> int:
    return int(conn.execute(f'SELECT COUNT(*) FROM "{table_name}"').fetchone()[0])


def rebuild_equipment_img_scraping(conn: sqlite3.Connection) -> None:
    """
    equipment_img_scraping ã‚’ (è£…å‚™å, ãƒ¬ã‚¢ãƒªãƒ†ã‚£) ã§1ä»¶ã«åœ§ç¸®ã—ã¦ç½®ãæ›ãˆã‚‹ã€‚
    URL_Number ãŒå°ã•ã„ã‚‚ã®ã‚’æ¡ç”¨ã€‚
    """
    # æ—¢å­˜ new_ ãŒæ®‹ã£ã¦ãŸã‚‰æ¶ˆã™
    if table_exists(conn, "new_equipment_img_scraping"):
        conn.execute('DROP TABLE "new_equipment_img_scraping"')

    conn.execute(
        """
CREATE TABLE "new_equipment_img_scraping" AS
WITH ranked AS (
  SELECT
    *,
    ROW_NUMBER() OVER (
      PARTITION BY "è£…å‚™å", "ãƒ¬ã‚¢ãƒªãƒ†ã‚£"
      ORDER BY "URL_Number" ASC
    ) AS rn
  FROM "equipment_img_scraping"
  WHERE "è£…å‚™å" IS NOT NULL
)
SELECT
  "è£…å‚™å",
  "ãƒ¬ã‚¢ãƒªãƒ†ã‚£",
  "ç”»åƒå",
  "ä½“åŠ›",
  "æ”»æ’ƒåŠ›",
  "é˜²å¾¡åŠ›",
  "ä¼šå¿ƒç‡",
  "å›é¿ç‡",
  "å‘½ä¸­ç‡",
  "ã‚¢ãƒ“ãƒªãƒ†ã‚£",
  "æ–°è¦ãƒ•ãƒ©ã‚°",
  "URL_Number",
  "IMG_URL",
  "IMG_Path",
  "BASE64"
FROM ranked
WHERE rn = 1
ORDER BY "URL_Number" ASC;
"""
    )

    # ç½®ãæ›ãˆ
    conn.execute('DROP TABLE "equipment_img_scraping"')
    conn.execute('ALTER TABLE "new_equipment_img_scraping" RENAME TO "equipment_img_scraping"')


def vacuum(conn: sqlite3.Connection) -> None:
    """
    VACUUM ã¯ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å¤–ã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ãŒå¤šã„ã®ã§ã€
    commitå¾Œã«å®Ÿè¡Œã™ã‚‹ã€‚
    """
    conn.execute("VACUUM")


def normalize_load_log_csv(log_path: Path, db_path: Path) -> None:
    """
    load_log.csv ã‚’é›†ç´„ã—ã¦ç½®ãæ›ãˆã‚‹ã€‚
    - group_keys ã« non_check_equipments ã‚’å«ã‚ã‚‹
    - CSVã« non_check_equipments åˆ—ãŒç„¡ã‘ã‚Œã°è¿½åŠ ï¼ˆâ€»ã‚ãªãŸã¯æ—¢ã«è¿½åŠ æ¸ˆã¿ã ãŒä¿é™ºï¼‰
    - ã•ã‚‰ã«ã€Œæœ€æ–°è¡Œã€ã® non_check_equipments ãŒ 0/ç©º ãªã‚‰ DBã‹ã‚‰è¡Œæ•°ã‚’å–ã£ã¦è£œå®Œ
    """
    if not log_path.exists() or log_path.stat().st_size == 0:
        print("load_log.csv ãŒè¦‹ã¤ã‹ã‚‰ãªã„/ç©ºãªã®ã§ä½•ã‚‚ã—ã¾ã›ã‚“")
        return

    df = pd.read_csv(log_path, encoding="utf-8-sig")
    if len(df) == 0:
        print("load_log.csv ãŒç©ºãªã®ã§ä½•ã‚‚ã—ã¾ã›ã‚“")
        return

    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
    must = ["æ›´æ–°æ—¥æ™‚", "ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"]
    missing_must = [c for c in must if c not in df.columns]
    if missing_must:
        raise ValueError(f"load_log.csv ã«å¿…é ˆåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing_must}")

    # non_check_equipments åˆ—ãŒç„¡ã‘ã‚Œã°ä½œã‚‹
    if "non_check_equipments" not in df.columns:
        df["non_check_equipments"] = 0

    # æ›´æ–°æ—¥æ™‚ã‚’datetimeåŒ–
    df["æ›´æ–°æ—¥æ™‚_dt"] = pd.to_datetime(df["æ›´æ–°æ—¥æ™‚"], errors="coerce")

    # ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚­ãƒ¼
    group_keys = [
        "uræ­¦å™¨", "uré˜²å…·", "urè£…é£¾",
        "ksræ­¦å™¨", "ksré˜²å…·", "ksrè£…é£¾",
        "ssræ­¦å™¨", "ssré˜²å…·", "ssrè£…é£¾",
        "ability_category",
        "non_check_equipments",
    ]

    # æ¬ ã‘ã¦ã‚‹åˆ—ã¯ã“ã“ã§åˆ†ã‹ã‚‹ã‚ˆã†ã«è½ã¨ã™
    missing = [c for c in group_keys if c not in df.columns]
    if missing:
        raise ValueError(f"load_log.csv ã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}")

    # â˜… è¿½åŠ ï¼šæœ€æ–°è¡Œã® non_check_equipments ãŒ 0/ç©ºãªã‚‰ DBã®è¡Œæ•°ã§è£œå®Œã—ã¦ãŠã
    # ï¼ˆã€Œæ›´æ–°ãŒã‚ã£ãŸéš›ã« non_check ã®è¡Œæ•°ãŒå‡ºåŠ›ã•ã‚Œãªã„ã€å•é¡Œã®æ•‘æ¸ˆï¼‰
    try:
        # ç©ºæ–‡å­—ãªã©ã‚’æ•°å€¤åŒ–ï¼ˆã§ããªã„ã‚‚ã®ã¯NaNï¼‰
        df["non_check_equipments"] = pd.to_numeric(df["non_check_equipments"], errors="coerce")
        last_idx = df["æ›´æ–°æ—¥æ™‚_dt"].sort_values().index[-1]
        last_val = df.loc[last_idx, "non_check_equipments"]
        if pd.isna(last_val) or int(last_val) == 0:
            conn = sqlite3.connect(str(db_path))
            if table_exists(conn, "non_check_equipments"):
                df.loc[last_idx, "non_check_equipments"] = count_rows(conn, "non_check_equipments")
            conn.close()
    except Exception:
        # è£œå®Œã§è½ã¡ã‚‹ã®ã¯æœ¬æœ«è»¢å€’ãªã®ã§ç„¡è¦–ï¼ˆé›†ç´„è‡ªä½“ã¯ç¶šè¡Œï¼‰
        pass

    # grouped_log: åŒä¸€ã‚­ãƒ¼ã”ã¨ã«æœ€å°æ›´æ–°æ—¥æ™‚ + ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å›ºå®š
    grouped = (
        df.groupby(group_keys, dropna=False)
          .agg({"æ›´æ–°æ—¥æ™‚_dt": "min"})
          .reset_index()
    )
    grouped["æ›´æ–°æ—¥æ™‚"] = grouped["æ›´æ–°æ—¥æ™‚_dt"].dt.strftime("%Y-%m-%d %H:%M:%S")
    grouped["ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"] = "No difference aggregated"

    # latest_log: æ›´æ–°æ—¥æ™‚ãŒæœ€å¤§ã®è¡Œã‚’1ä»¶ï¼ˆå…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ®‹ã™ï¼‰
    latest = (
        df.sort_values("æ›´æ–°æ—¥æ™‚_dt", ascending=True)
          .tail(1)
          .drop(columns=["æ›´æ–°æ—¥æ™‚_dt"])
    )

    out_cols = ["æ›´æ–°æ—¥æ™‚", "ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"] + group_keys
    new_df = pd.concat([grouped[out_cols], latest[out_cols]], ignore_index=True)

    new_df["æ›´æ–°æ—¥æ™‚_dt"] = pd.to_datetime(new_df["æ›´æ–°æ—¥æ™‚"], errors="coerce")
    new_df = (
        new_df.sort_values("æ›´æ–°æ—¥æ™‚_dt", ascending=True)
              .drop(columns=["æ›´æ–°æ—¥æ™‚_dt"])
              .reset_index(drop=True)
    )

    # ç½®æ›ï¼ˆå®‰å…¨ã®ãŸã‚ tmp ã«æ›¸ã„ã¦å·®ã—æ›¿ãˆï¼‰
    tmp_path = log_path.with_suffix(".tmp.csv")
    new_df.to_csv(tmp_path, index=False, encoding="utf-8-sig")
    tmp_path.replace(log_path)

    print("ğŸ“ load_log.csv ã‚’é›†ç´„ã—ã¦ç½®ãæ›ãˆã¾ã—ãŸ")


def main() -> None:
    # DBåŠ å·¥ï¼ˆequipment_img_scraping ã®é‡è¤‡æ’é™¤ï¼‰
    conn = sqlite3.connect(str(DB_PATH))
    try:
        rebuild_equipment_img_scraping(conn)
        conn.commit()
        vacuum(conn)  # commitå¾Œã«å®Ÿè¡Œï¼ˆå®‰å…¨ï¼‰
        conn.commit()
    finally:
        conn.close()

    # ãƒ­ã‚°CSVã®é›†ç´„ãƒ»è£œå®Œ
    normalize_load_log_csv(LOG_PATH, DB_PATH)


if __name__ == "__main__":
    main()
